# This datafile documents all the possible optimizations that the optimizer can/should do.
# It is parsed by the parser described in fpoptimizer_grammar_gen.y, which
# is compiled into C++ code in fpoptimizer_grammar_gen.cc. The parser produces
# a C++ file, fpoptimizer_grammar.cc , which lists the grammar rules in tabular
# format. The grammar rules are utilized by fpoptimizer_optimize.cc , which
# matches the function trees into the rules and performing those replacements
# which can be performed.
#
# Copyright 2009 Joel Yliluoma, written specifically
#               for Warp's Function Parser (fparser).
#

# Substitution rule syntax:
#
# %token NUMERIC_CONSTANT     # literals such as 0, 1, 1.5 or CONSTANT_DR, CONSTANT_L10I
# %token NAMEDHOLDER_TOKEN    # placeholders such as x, y, a, b
# %token RESTHOLDER_TOKEN     # placeholders such as <1>, <2>, <7>
# %token IMMEDHOLDER_TOKEN    # placeholders % and &
# %token BUILTIN_FUNC_NAME    # such as COS, CEIL, POW, +, *, MIN, MAX
# %token OPCODE               # opcodes such as cCos, cMul
# %token UNARY_TRANSFORMATION # /, -, !  # inverts/negates/inverts the param
# %token PARAM_CONSTRAINT     # parameter constraint specifier:
#                                 @E  = Even integer only
#                                 @O  = Odd integer only
#                                 @I  = Integer only
#                                 @F  = (Float) non-integer only
#                                 @L  = Logical operation (something that only yields 0 or 1)
#                                 @P  = Positive only (also zero)
#                                 @N  = Negative only
#                                 @Q  = Only those where positive/negative not known
#                                 @1  = value evaluating to +1 or -1 only
#                                 @M  = (Multiple) value NOT evaluating to +1 or -1
#                             # they can be applied to IMMEDHOLDER_TOKENs and NAMEDHOLDER_TOKENs
# %token NEWLINE              # newline
#
# %%
#     grammar:
#       grammar substitution
#     | grammar NEWLINE
#     | /* empty */
#     ;
#
#     substitution:
#       function '->' param NEWLINE
#       /* Entire function is changed into the particular param */
#
#     | function '->' function NEWLINE
#       /* Entire function changes, the param_notinv_list is rewritten */
#       /* NOTE: "p x -> o y"  is a shortcut for "p x -> (o y)"  */
#
#     | function ':'  paramlist NEWLINE
#       /* The params provided are replaced with the new param_maybeinv_list */
#     ;
#
#     function:
#        OPCODE '[' paramlist ']'
#        /* Match a function with opcode=opcode,
#         * and the exact parameter list as specified
#         */
#        OPCODE '{' paramlist '}'
#        /* Match a function with opcode=opcode,
#         * and the exact parameter list in any order
#         */
#     |  OPCODE paramlist
#        /* Match a function with opcode=opcode and the given way of matching params */
#        /* There may be more parameters, don't care about them */
#     ;
#
#     paramlist: /* left-recursive list of 0-n params with no delimiter */
#       | paramlist param             /* param */
#       | paramlist RESTHOLDER_TOKEN  /* a placeholder for all remaining params */
#       | /* empty */
#     ;
#
#     param:
#        NUMERIC_CONSTANT                     /* particular immed */
#     |  IMMEDHOLDER_TOKEN param_constraints  /* a placeholder for some immed */
#     |  BUILTIN_FUNC_NAME '(' paramlist ')'  /* literal logarithm/sin/etc. of the provided immed-type params -- also sum/product/minimum/maximum */
#     |  NAMEDHOLDER_TOKEN param_constraints  /* any expression, indicated by "x", "a" etc. */
#     |  (' function ')' param_constraints    /* a subtree */
#     |  UNARY_TRANSFORMATION param           /* the negated/inverted literal value of the param */
#     ;
#
#     param_constraints: /* List of possible constraints to the given param, eg. odd,int,etc */
#        param_constraints PARAM_CONSTRAINT
#     |  /* empty */
#     ;

[BASIC]

###### Note: Before adding new rules (especially those which handle constant values),
######       verify that it is not already done in ConstantFolding().

#### Remove redundant components:

# In a min-list, two identical components are reduced into one
cMin x x    : x
# In a max-list, two identical components are reduced into one
cMax x x    : x

# If both branches of an If() are identical, the test becomes unnecessary
cIf [y x x] -> x

# There are more optimizations that could be done with If(),
# but they are likely too specific.
# For example:
#  Move branch invariants. if(y, x+a, x+b) -> x+if(y, a, b)
cIf [y (cAdd x <1>)       x     ] -> cAdd [x (cIf [y (cAdd <1>) 0         ])]
cIf [y       x      (cAdd x <1>)] -> cAdd [x (cIf [y 0          (cAdd <1>)])]
cIf [y (cAdd x <1>) (cAdd x <2>)] -> cAdd [x (cIf [y (cAdd <1>) (cAdd <2>)])]

cIf [y (cMul x <1>)       x     ] -> cMul [x (cIf [y (cMul <1>) 1         ])]
cIf [y       x      (cMul x <1>)] -> cMul [x (cIf [y 1          (cMul <1>)])]
cIf [y (cMul x <1>) (cMul x <2>)] -> cMul [x (cIf [y (cMul <1>) (cMul <2>)])]

cIf [y (cAnd x <1>) (cNotNot[x])] -> cAnd [x (cIf [y (cAnd <1>) 1         ])]
cIf [y (cAnd x@L <1>) x@L       ] -> cAnd [x (cIf [y (cAnd <1>) 1         ])]
cIf [y (cNotNot[x]) (cAnd x <1>)] -> cAnd [x (cIf [y 1          (cAnd <1>)])]
cIf [y x@L        (cAnd x@L <1>)] -> cAnd [x (cIf [y 1          (cAnd <1>)])]
cIf [y (cAnd x <1>) (cAnd x <2>)] -> cAnd [x (cIf [y (cAnd <1>) (cAnd <2>)])]

cIf [y (cOr  x <1>) (cNotNot[x])] -> cOr  [x (cIf [y (cOr  <1>) 0         ])]
cIf [y (cOr  x@L <1>) x@L       ] -> cOr  [x (cIf [y (cOr  <1>) 0         ])]
cIf [y (cNotNot[x]) (cOr  x <1>)] -> cOr  [x (cIf [y 0          (cOr  <1>)])]
cIf [y x@L         (cOr x@L <1>)] -> cOr  [x (cIf [y 0          (cOr  <1>)])]
cIf [y (cOr  x <1>) (cOr  x <2>)] -> cOr  [x (cIf [y (cOr  <1>) (cOr  <2>)])]

cIf [(cGreater[x y])     x y] -> cMax x y
cIf [(cGreaterOrEq[x y]) x y] -> cMax x y
cIf [(cLess[x y])        x y] -> cMin x y
cIf [(cLessOrEq[x y])    x y] -> cMin x y
cIf [(cGreater[x y])     y x] -> cMin x y
cIf [(cGreaterOrEq[x y]) y x] -> cMin x y
cIf [(cLess[x y])        y x] -> cMax x y
cIf [(cLessOrEq[x y])    y x] -> cMax x y
cIf [(cGreater[0 x]) (cCeil[x]) (cFloor[x])] -> cTrunc x
cIf [(cLess[0 x])    (cFloor[x]) (cCeil[x])] -> cTrunc x

# ceil(-x) = -floor(x); floor(-x) = -ceil(x)
cFloor[(cMul -1 <1>)] -> cMul -1 (cCeil[(cMul <1>)])
cCeil[(cMul -1 <1>)] -> cMul -1 (cFloor[(cMul <1>)])

#cIf [x y@L 0] -> cAnd x y
#cIf [x 0 y@L] -> cAnd (cNot[x]) y
#cIf [x y 0] -> cMul (cNotNot[x]) y
#cIf [x 0 y] -> cMul (cNot[x])    y
# These cannot be done because y may have side
# effects or just be computation-heavy.

cMul (cNot[x])    (cNotNot[y]) : (cAnd (cNot[x]) y)
cMul (cNot[x])    (cNot[y])    : (cNot (cOr x y))
cMul (cNotNot[x]) (cNotNot[y]) : (cAnd x y)
# ^Warning: There is probably chance for infinite loop in those rules
#cAnd (cNot[x]) (cNot[y]) : (cNot [(cOr x y)])
# ^causes crash.

cNot [(cMul {-1 x})] : x

#### Flattening the topology of add/mul/min/max/and/or groups:

#### Logarithm optimizations
# log(x^y) = y*log(x)
cLog [(cPow [x@P y])] -> cMul y (cLog[x])

# log(x^e) = e * log(abs(x))   (e is an even integer)
cLog [(cPow [x y@E])] -> cMul y (cLog [(cAbs [x])])

# CONSTANT_E^log(x) = x
# CONSTANT_E^(log(x)*y) = x^y
# Generalized as:  p^ log(x)    = x^ log(p)
#                  p^(log(x)*y) = x^(log(p)*y)
#
# Warning: This loses the information that x > 0,
#          that could be utilized in further optimizations.
#
cPow [%       (cLog[x])     ]   :     x        LOG(%)
cPow [% (cMul (cLog[x]) <1>)]   :     x  (cMul LOG(%) <1>)

# Because log(exp(6)*x) = log(x)+6, we can also do this:
#                  y^(log(x)+z)
#                = y^(log(x*exp(z)))
#                = (x*exp(z))^log(y)
#                = x^log(y) * y^z
#cPow [y (cAdd {(cLog[x]) &})] -> cMul (cPow [y &]) (cPow [x (cLog [y])])
#
# Probably beneficial to do it only when y is const,
# though. Otherwise we only trade + for *, which is bad.
# Also z should be const, otherwise we get two pows instead of one.
cPow [% (cAdd {(cLog[x]) &})]              -> cMul POW(% &) (cPow [x LOG(%)])

# x^(y*z) = (x*y)^z - done by ConstantFolding

# z^(log(x)/log(z)*y) = x^y
# Note: This rule fails when z=0, because log(0)=-inf and 0^x = 1
cPow [z (cMul (cPow [(cLog[z])  -1]) (cLog[x])  <1>)] : x (cMul <1>)
cPow [% (cMul /LOG(%)                (cLog[x])  <1>)] : x (cMul <1>)

# (x*5) ^ 2 = x^2 * (5^2)
# Note: This also applies to (x*5)^-1 --> 0.2/x , shrug
cPow [(cMul % <1>) &]                      -> cMul POW(% &)   (cPow [(cMul <1>) &])

# z^(x+y/log(z)) = z^x * exp(y)
# Note: This rule fails when z=0, because log(0)=-inf and 0^z = 1
cPow [z (cAdd <1> (cMul <2> (cPow [(cLog [z]) -1])))] -> cMul (cPow [z (cAdd <1>)]) (cPow [CONSTANT_E (cMul <2>)])
#cPow [% (cAdd <1> (cMul <2> /LOG(%)))]    -> cMul (cPow [% (cAdd <1>)]) (cPow [CONSTANT_E (cMul <2>)])
cPow [z (cAdd <1> (cPow [(cLog [z]) -1]))] -> cMul CONSTANT_E (cPow [z (cAdd <1>)])
cPow [% (cAdd <1> &@M)]                    -> cMul POW(% &)   (cPow [% (cAdd <1>)])

# x*y / (x*z) = y/z
cMul (cPow [(cMul x <2>) -1]) x : (cPow [(cMul <2>) -1])

# log(x) + log(y) = log(x*y)
cAdd (cLog[x])  (cLog[y])  : (cLog  (cMul [x y]))
# When x is const, the reverse is more beneficial
#  i.e.  log(2*x) = log(x) + log(2)
cLog  [(cMul %@P <1>)] -> cAdd (cLog  [(cMul <1>)]) LOG(%)

# log(x * z^y) = (log(x) / log(z) + y) * log(z)
#              = log(x) + log(z)*y
# Only worthwhile when z is an immed, otherwise we trade cPow for cLog
# Note that when z = CONSTANT_E, this reduces rather nicely into log(x) + y
cLog  [(cMul (cPow [% y]) <1>)]  -> cAdd (cMul [LOG(%) y]) (cLog [(cMul <1>)])

#cLog  [(cMul (cPow [% y]) <1>)] -> cMul LOG(%) (cAdd [y (cMul (cLog [(cMul <1>)]) /LOG(%))])
# When y=1, the reverse is more useful:
cMul {% (cAdd {1 (cMul {(cLog  [x]) /%})})} -> cAdd (cLog  [x]) %


#### Linear combining and polynomials

# If a mul-list contains the same element two or more times,
# replace the element with a pow with an integer exponent
# (note: inversions are treated as a negative exponent)
# e.g. ...*x*x*x*x = ...*x^4
# x*2 + x*5 = x*(2+5)
# All done by ConstantFolding()

#### Remote double negations/inversions

# (x^g)^z        -> x^(g*z)  (g is odd or float)
cPow [ (cPow[x y@O]) z ]       : x (cMul [y z])
cPow [ (cPow[x y@F]) z ]       : x (cMul [y z])

# (x^y)^z where y is positive can be changed safely into x^(y*z)
cPow [(cPow [x@P y]) z]        : x (cMul [y z])
# (x^y)^z where (y*z) makes an even integer could also be changed safely
#cPow [(cPow [x y@E]) z@I]        : x (cMul [y z])
#cPow [(cPow [x y@I]) z@E]        : x (cMul [y z])

# If pow() makes a signless value into a positive value, guard that fact with abs()
cPow [ (cPow[x@Q y])@P z ]     : (cAbs [x]) (cMul [y z])
# abs(x)^e -> x^e when e=even integer
# This removes the abs() generated by the above rule when needless
cPow [(cAbs[x]) y@E] : x y
cPow [(cMul (cAbs[x]) <1>) y@E] : (cMul x <1>) y

#### Logical optimizations

# From logic, follows that...
#   (a==b) & (b==c) & (a==c) -- one of these is redundant
cAnd (cEqual[x y]) (cEqual[y z]) (cEqual[x z])  : (cEqual[x y]) (cEqual[y z])
# Note: ^ Replacement function refers to b twice

# !x = abs(x) < 0.5
# Note: Due to range-based optimizations, % can never be 0 here. These are safe.
cLess        [(cAbs[x]) %] -> cNot[(cMul x 0.5 /%)]
cGreaterOrEq [(cAbs[x]) %] -> cNotNot[(cMul x 0.5 /%)]
cGreater     [% (cAbs[x])] -> cNot[(cMul x 0.5 /%)]
cLessOrEq    [% (cAbs[x])] -> cNotNot[(cMul x 0.5 /%)]

# Contradictory comparisons always produce both 0 and 1
#  These are done by ConstantFolding:
#cAdd (cLess[x y]) (cGreaterOrEq[x y]) : 1
#cAdd (cLessOrEq[x y]) (cGreater[x y]) : 1
#cAdd (cEqual[x y]) (cNEqual[x y])     : 1
#cAdd (cNot[x]) (cNotNot[x])           : 1
#cMul (cLess[x y]) (cGreaterOrEq[x y]) -> 0
#cMul (cLessOrEq[x y]) (cGreater[x y]) -> 0
#cMul (cEqual[x y]) (cNEqual[x y])     -> 0
#cMul (cNot[x]) (cNotNot[x])           -> 0
## Comparisons that have overlap can be reduced
#cAdd (cLessOrEq[x y]) (cGreaterOrEq[x y])  : 1 (cEqual[x y])
## equal*2 + less
##cAdd (cLessOrEq[x y]) (cEqual[x y])
#cMul (cLessOrEq[x y]) (cGreaterOrEq[x y])  : (cEqual[x y])
#cMul (cEqual[x y])    (cGreaterOrEq[x y])  : (cEqual[x y])
#cMul (cEqual[x y])    (cLessOrEq[x y])     : (cEqual[x y])

[INTERMEDIATE]

# Opcodes we will NOT find in the intermediate stage:
#  Done by bytecode parser:
#   Meta opcodes: cDup, cNop, cFetch, cPopNMov, cJump
#   Meta opcodes: cVar, cImmed
#   Implemented through cMul: cDiv, cRDiv, cInv, cSqr
#   Implemented through cAdd: cSub, cRSub, cNeg
#   Implemented through constant-multiplying: cDeg, cRad
#   Implemented through cSin, cCos: cCot, cCsc, cSec, cTan
#   Implemented through cPow: cSqrt, cExp
#   Implemented through cLog: cLog2, cLog10
#  Done by entry rules:
#   Extracted: cAsinh, cAcosh, cAtanh
#   Extracted: cSinh, cCosh, cTanh

#### CONTINUED: Flattening the topology of add/mul/min/max/and/or groups

# a^2 + a*b*X/Z + b^2 = (a+b)^2 + (X/Z-2)*(a*b)
cAdd (cPow[x 2]) (cPow[y 2]) (cMul x y <1>) : (cPow [(cAdd [x y]) 2]) (cMul [x y (cAdd [(cMul <1>) -2])])
# For optimizing x^2+2*x*y+y^2:
#  With this rule,    eval=0.287154 us, optimized = 0.0758879 us
#  Without this rule, eval=0.314538 us, optimized = 0.0831386 us
# For optimizing x^2+3*x*y+y^2:
#  With this rule,    eval=0.295956 us, optimized = 0.0781288 us
#  Without this rule, eval=0.300723 us, optimized = 0.075689 us
# The benchmark results seem too varying, so it is hard to tell
# whether this rule had some advantage. It _looks_ like it did
# though, so better keep it, I suppose. -Bisqwit
#
# How about this?
# (a+b+c)^2 = c^2 + 2*b*c + 2*a*c + b^2 + 2*a*b + a^2
# Seems that it becomes:
# a^2 + b^2 + c^2 + 2*((a+b)*c + a*b)
# Is it worth adding rule for making that into (a+b+c)^2?
# Too specific, I suppose.

# These are the same as above, but work also if pow() is expanded
# Note: It would work even with y and z instead of % and &, but we
# limit into numeric literals for simplicity.
cAdd (cMul (cPow[x %@P@I]) <1>)  (cMul (cPow[x &@I]) <2>) : (cMul (cPow[x MIN(% &)]) (cAdd (cMul <1> (cPow[x (cAdd % -MIN(% &))]))  (cMul <2> (cPow[x (cAdd & -MIN(% &))]))))

# Note:
#   x^4*a + b*x^9  -> (x^5 * b + a)*x^4:  Eval time goes 0.046 -> 0.056
#   x^5*a + b*x^11 -> (x^6 * b + a)*x^5:  Eval time goes 0.060 -> 0.049
#     srsly, what?


cAdd (cMul (cPow[x %@P@I]) <1>)  (cMul x           <2>) : (cMul (cPow[x MIN(% 1)]) (cAdd (cMul <1> (cPow[x (cAdd % -MIN(% 1))]))  (cMul <2> (cPow[x (cAdd 1 -MIN(% 1))]))))
# The replacement expanded below:
#  (cMul (cPow[x MIN(% 1)])
#        (cAdd (cMul <1> (cPow[x (cAdd % -MIN(% 1))]))
#              (cMul <2> (cPow[x (cAdd 1 -MIN(% 1))]))))
#
# Example: x^2*y  + x*z -> x^1  * (y*x^1 + z*x^0)
# Example: x^6*y  + x*z -> x^1  * (y*x^5 + z*x^0)
# Example: x^-6*y + x*z -> x^-6 * (y*x^0 + z*x^7) -- not good, so restricted with @P
#
# Example: x*z + 2*x^0.7 -> x^0.7 * (x^0.3 * z+2) -- not good, so also restricted with @I
#


#    (5.1*x +      4.1*y      + z+w)*2
# -> (5.1*2*x + 2*(4.1*y      + z+w))
# -> (5.1*2*x +   (4.1*2*y + 2*(z+w)))
cMul (cAdd (cMul %@M <1>) <2>) &  :  (cAdd (cMul % & <1>) (cMul & (cAdd <2>)))

#    (2+x+y)*4 = 2*4 + 4*(x+y)
cMul (cAdd % <1>) &  :  (cAdd *(% &) (cMul & (cAdd <1>)))

#### Constant folding is now performed exclusively by ConstantFolding()

#### Trigonometric:
# sin(-x) = -sin(x)
cSin [(cMul %@N <1>)] -> cMul -1 (cSin [(cMul -% <1>)])
# cos(-x) = cos(x)
cCos [(cMul %@N <1>)] : (cMul -% <1>)


# cos(pi/2 - x) = sin(x)
cCos [(cAdd {CONSTANT_PIHALF (cMul %@N <1>)})] -> cSin[(cMul -% <1>)]
# sin(pi/2 - x) = cos(x)
cSin [(cAdd {CONSTANT_PIHALF (cMul %@N <1>)})] -> cCos[(cMul -% <1>)]
# cos(x - pi/2) = cos(pi/2 - x) = sin(x)
cCos [(cAdd -CONSTANT_PIHALF <1>)]            -> cSin[(cAdd <1>)]
# sin(x - pi/2) = -sin(pi/2 - x) = -cos(x)
cSin [(cAdd -CONSTANT_PIHALF <1>)]            -> cMul -1 (cCos[(cAdd <1>)])

# sin(x)^2 + cos(x)^2 = 1
cAdd  (cPow[ (cSin[x]) 2]) (cPow [(cCos[x]) 2]) : 1
# y-sin(x)^2 = cos(x)^2+(y-1)
# y-cos(x)^2 = sin(x)^2+(y-1)
cAdd 1 (cMul { -1 (cPow[ (cSin[x]) 2]) }) : (cPow [(cCos[x]) 2])
cAdd 1 (cMul { -1 (cPow[ (cCos[x]) 2]) }) : (cPow [(cSin[x]) 2])

# sin(x)*cos(y) + cos(x)*sin(y) = sin(x+y)
# sin(x)*cos(y) - cos(x)*sin(y) = sin(x-y)
# cos(x)*cos(y) + sin(x)*sin(y) = cos(x+y)
# cos(x)*cos(y) - sin(x)*sin(y) = cos(x-y)

cAdd  (cMul {(cSin[x]) (cCos[y])}) (cMul {(cCos[x]) (cSin[y])   }) :  (cSin [(cAdd[x           y]  )])
cAdd  (cMul {(cSin[x]) (cCos[y])}) (cMul {(cCos[x]) (cSin[y]) -1}) :  (cSin [(cAdd[x (cMul [-1 y])])])
cAdd  (cMul {(cCos[x]) (cCos[y])}) (cMul {(cSin[x]) (cSin[y])   }) :  (cCos [(cAdd[x           y]  )])
cAdd  (cMul {(cCos[x]) (cCos[y])}) (cMul {(cSin[x]) (cSin[y]) -1}) :  (cCos [(cAdd[x (cMul [-1 y])])])

#cAdd  (cMul {(cSin[x]) (cCos[y]) -1}) (cMul {(cCos[x]) (cSin[y]) -1}) : (cMul [-1 (cSin [(cAdd[x           y]  )]) ])
#cAdd  (cMul {(cCos[x]) (cCos[y]) -1}) (cMul {(cSin[x]) (cSin[y]) -1}) : (cMul [-1 (cCos [(cAdd[x           y]  )]) ])
# ^This one is redundant, subexpression grouping already catches it
cAdd  (cMul {(cCos[x]) (cCos[y]) -1}) (cMul {(cSin[x]) (cSin[y])   }) : (cMul [-1 (cCos [(cAdd[x (cMul [-1 y])])]) ])
#cAdd (cMul {(cSin[x]) (cCos[y]) -1}) (cMul {(cCos[x]) (cSin[y])   }) : (cMul [-1 (cSin [(cAdd[x (cMul [-1 y])])]) ])
# ^This one is redudant: It just reaffirms that sin(x) = -sin(-x).


#### Self-defeating function calls:

# sin(asin(x)) = x
cSin [(cAsin [x])] -> x

# cos(acos(x)) = x
cCos [(cAcos [x])] -> x

# Note: asin(sin(x)) must not be converted, because
# even though asin(sin(1.1)) = 1.1, asin(sin(1500)) != 1500.

cMul (cAbs[x]) (cAbs[y]) : (cAbs[(cMul x y)])



[FINAL1]
# These optimizations are done only once, after the intermediate conversions.
# This process generates high-level opcodes that are not
# expected to be found in the intermediate stage.
#
# Do not generate the following opcodes here:
#     cDiv, cRDiv, cInv, cSub, cRSub, cNeg, cNot
#     cSqrt, cRSqrt, cExp
#     cCsc, cSec, cCot
# Do not reduce add/mul/min/max/and/or lists to two-operand topology.
# Those are done in the bytecode generation automatically.
#

cAtan2 [(cMul  x <1>) (cMul  x <2>)]   : (cMul <1>) (cMul <2>)
#cAtan2 [(cMul  % <1>) (cMul  % <2>)]   : (cMul <1>) (cMul <2>)

# sinh(x)/cosh(x) = tanh(x)
cMul        (cSinh[x])     (cPow [(cCosh[x]) -1]) :        (cTanh[x])
cMul (cPow [(cSinh[x]) -1])       (cCosh[x])      : (cPow [(cTanh[x]) -1])
cMul        (cTanh[x])            (cCosh[x])      :        (cSinh[x])
cMul        (cTanh[x])     (cPow [(cSinh[x]) -1]) : (cPow [(cCosh[x]) -1])
cMul (cPow [(cTanh[x]) -1])       (cSinh[x])      :        (cCosh[x])

# sin(x)/cos(x) = tan(x)
cMul        (cSin[x])     (cPow [(cCos[x]) -1]) :        (cTan[x])
cMul (cPow [(cSin[x]) -1])       (cCos[x])      : (cPow [(cTan[x]) -1])
cMul        (cTan[x])            (cCos[x])      :        (cSin[x])
cMul        (cTan[x])     (cPow [(cSin[x]) -1]) : (cPow [(cCos[x]) -1])
cMul (cPow [(cTan[x]) -1])       (cSin[x])      :        (cCos[x])

# cos(x)^(-2) * sin(x) = tan(x)/cos(x)
# sin(x)^2    / cos(x) = tan(x)*sin(x)


# sinh(-5*x) / cosh(5*x) = tanh(-5*x)
#cMul (cSinh [(cMul % <1>)]) (cPow [(cCosh [(cMul -% <1>)]) -1]) : (cTanh [(cMul % <1>)])
#cMul (cSin  [(cMul % <1>)]) (cPow [(cCos  [(cMul -% <1>)]) -1]) : (cTan  [(cMul % <1>)])
#^ DISABLED: MULTIPLE/REPEATED RESTHOLDERS


# sinh(-x) = -sinh(x)
cSinh [(cMul -1 <1>)] -> cMul [-1 (cSinh [(cMul <1>)])]

# cosh(-x) = cosh(x)
cCosh [(cMul -1 <1>)] : (cMul <1>)

# tan(-x) = -tan(x)
cTan [(cMul -1 <1>)] -> cMul [-1 (cTan [(cMul <1>)])]

# tanh(-x) = -tanh(x)
cTanh [(cMul -1 <1>)] -> cMul [-1 (cTanh [(cMul <1>)])]

# However,  -sinh(5*x) better expressed as sinh(-5*x), same for tanh
cMul -1 (cSinh [(cMul % <1>)]) : (cSinh [(cMul -% <1>)])
cMul -1 (cTanh [(cMul % <1>)]) : (cTanh [(cMul -% <1>)])
cMul -1 (cSin [(cMul % <1>)]) : (cSin [(cMul -% <1>)])
cMul -1 (cTan [(cMul % <1>)]) : (cTan [(cMul -% <1>)])


# sinh(x)*2 = (exp(x)-   exp(-x))
# sinh(x)*2 = (exp(x)- 1/exp(x))
# cosh(x)*2 = (exp(x)+   exp(-x))
# cosh(x)*2 = (exp(x)+ 1/exp(x))
cAdd (cPow [&  x]) (cMul { -1 (cPow [/& x]) }) : (cMul (cSinh [(cMul x LOG(&))]) 2)
#cAdd (cMul { -1 (cPow [& x]) }) (cPow [/&  x]) : (cMul (cSinh [(cMul x LOG(/&))]) 2)
cAdd (cPow [&  x])            (cPow [/& x])    : (cMul (cCosh [(cMul x LOG(&))]) 2)

# sinh(x)   = ((((E^2) ^ x) + -1) * ((E^-1) ^ x) * 0.5)
# sinh(3*x) = ((((E^6) ^ x) + -1) * ((E^-3) ^ x) * 0.5)

# sinh(3*x)*2              = (((E^6) ^ x) + -1) * ((E^-3) ^ x)
# sinh(3*x)*2 / (E^-3) ^x = (((E^6) ^ x) + -1)
# sinh(3*x)*2 * (E^ 3) ^x = (((E^6) ^ x) + -1)

#cAdd {(cPow [% x]) -1}  : (cMul (cSinh [(cMul x LOG(%) 0.5)]) 2 (cPow [% (cMul x 0.5)]))
#cAdd {(cPow [% x])  1}  : (cMul (cCosh [(cMul x LOG(%) 0.5)]) 2 (cPow [% (cMul x 0.5)]))

#cMul (cAdd {(cPow [% x]) -1}) (cPow [POW(% -0.5) x]) : (cSinh [(cMul x LOG(%) 0.5)]) 2
#cMul (cPow [% x]) (cAdd {(cPow [POW(% -2) x]) -1})   : (cSinh [(cMul x LOG(%) 0.5)]) 2

# Because sinh(-x) = -sinh(x),
# sinh(x)*-2 = (exp(-x)-exp(x))
cAdd (cMul {-1 (cPow [& x])})            (cPow [/& x])   : (cMul (cSinh [(cMul x LOG(&))]) -2)
cAdd (cMul {% (cPow [& x])})  (cMul { -% (cPow [/& x])}) : (cMul (cSinh [(cMul x LOG(&))]) 2 %)
cAdd (cMul {% (cPow [& x])})  (cMul {  % (cPow [/& x])}) : (cMul (cCosh [(cMul x LOG(&))]) 2 %)

#        tanh(x) = (exp(2*x)-1) / (exp(2*x)+1)
#      1/tanh(x) = (exp(2*x)+1) / (exp(2*x)-1)
#        tanh(-x) = -tanh(x), so
#       -tanh(x) = (exp(-2*x)-1) / (exp(-2*x)+1)
#     1/-tanh(x) = (exp(-2*x)+1) / (exp(-2*x)-1)
cMul (cAdd {(cPow [CONSTANT_2EI x]) -1}) (cPow [(cAdd {(cPow [CONSTANT_2EI x])  1}) -1]) : -1 (cTanh [x])
cMul (cAdd {(cPow [CONSTANT_2E  x]) -1}) (cPow [(cAdd {(cPow [CONSTANT_2E  x])  1}) -1]) : (cTanh [x])
cMul (cAdd {(cPow [CONSTANT_2EI x])  1}) (cPow [(cAdd {(cPow [CONSTANT_2EI x]) -1}) -1]) : (cPow [(cMul [-1 (cTanh [x])]) -1])
cMul (cAdd {(cPow [CONSTANT_2E  x])  1}) (cPow [(cAdd {(cPow [CONSTANT_2E  x]) -1}) -1]) : (cPow [(cTanh [x]) -1])

# Note: I have no idea why (exp(x)-exp(-x)) / (exp(x)+exp(-x))
#                 produces (exp(2*x)-1) / (exp(2*x)+1)
#       It just says so in Wikipedia.
#       Maybe this could be utilized for more generic optimizations?
#
# Maxima gives that exp(x) + n*exp(-x)
#                 = exp(-x) * (exp(2*x) + n)
# but I still have no idea why is that.
#
# In more generic form:
#   exp( d*y) * ((a * exp(b*x + -1*d*y)) + c)   = a*exp(    b*x) + c*exp( d*y)
#
#   exp(   y) * ((a * exp(  x + -1*  y)) + c)   = a*exp(      x) + c*exp(   y)
#   exp(   y) * ((    exp(  x + -1*  y)) + c)   =   exp(      x) + c*exp(   y)
#
#   exp( d*x) * ((a * exp(x*(b + -1*d))) + c)   = a*exp(    b*x) + c*exp( d*x)
#   exp(   x) * ((a * exp(x*b         )) + c)   = a*exp((b+1)*x) + c*exp(   x)
#   exp(   x) * ((a * exp(x           )) + c)   = a*exp(    2*x) + c*exp(   x)
# This is quite hairy.
# A rare special case too.
#
#cMul (cPow [CONSTANT_E y]) (cAdd <2> (cMul <1> (cPow [CONSTANT_E (cAdd <3> (cMul {-1 y}))]))) : (cAdd (cMul (cAdd <2>) (cPow [CONSTANT_E y])) (cMul <1> (cPow [CONSTANT_E (cAdd <3>)])))
#cMul (cPow [CONSTANT_E y]) (cAdd <2>           (cPow [CONSTANT_E (cAdd <3> (cMul {-1 y}))]))  : (cAdd (cMul (cAdd <2>) (cPow [CONSTANT_E y]))           (cPow [CONSTANT_E (cAdd <3>)]))
#cMul (cPow [CONSTANT_E x]) (cAdd <2> (cMul <1> (cPow [CONSTANT_E x]))) : (cAdd (cMul (cAdd <2>) (cPow [CONSTANT_E x])) (cMul <1> (cPow [CONSTANT_E (cMul [2 x])])))
#cMul (cPow [CONSTANT_E x]) (cAdd <2> (cMul <1> (cPow [CONSTANT_E (cMul x <3>)]))) : (cAdd (cMul (cAdd <2>) (cPow [CONSTANT_E x])) (cMul <1> (cPow [CONSTANT_E  (cMul [(cAdd 1 (cMul <3>)) x])])))


# exp(x)  = cosh(x)+sinh(x)
# exp(-x) = cosh(x)-sinh(x)
# -exp(-x) = sinh(x)-cosh(x)
cAdd        (cCosh [x])             (cSinh [x])      :        (cPow [CONSTANT_E  x])
cAdd        (cCosh [x])      (cMul {(cSinh [x]) -1}) :        (cPow [CONSTANT_EI x])
cAdd (cMul {(cCosh [x]) -1})        (cSinh [x])      : (cMul {(cPow [CONSTANT_EI x]) -1})
# -cosh(x) = sinh(x)-exp(x)
#  cosh(x) = exp(x)-sinh(x)
# -sinh(x) = cosh(x)-exp(x)
#  sinh(x) = exp(x)-cosh(x)
cAdd        (cSinh [x])      (cMul {(cPow [CONSTANT_E  x]) -1}) : (cMul -1 (cCosh [x]))
cAdd (cMul {(cSinh [x]) -1})        (cPow [CONSTANT_E  x])      :          (cCosh [x])
cAdd        (cCosh [x])      (cMul {(cPow [CONSTANT_E  x]) -1}) : (cMul -1 (cSinh [x]))
cAdd (cMul {(cCosh [x]) -1})        (cPow [CONSTANT_E  x])      :          (cSinh [x])

# sinh(acosh(x)) = sqrt(x^2 - 1)  (not a typo)
# cosh(asinh(x)) = sqrt(x^2 + 1)  (not a typo)
#  Not sure whether these are faster. They are more opcodes, but
#  simpler. The rationale is in allowing for further optimizations.
cSinh [(cAcosh [x])] -> cPow [(cAdd [(cPow [x 2]) -1]) 0.5]
cCosh [(cAsinh [x])] -> cPow [(cAdd [(cPow [x 2])  1]) 0.5]

# asin(tan(x)) = x / (1-x^2)^0.5
#cAsin [(cTan [x])] -> cMul x (cPow [(cAdd (cMul (cPow [x 2]) -1) 1) -0.5])
#
# ^Disabled: Incorrectly produces error when x = 1

# acos(tan(x)) = (1-x^2)^0.5 / x
#cAcos [(cTan [x])] -> cMul (cPow [x -1]) (cPow [(cAdd (cMul (cPow [x 2]) -1) 1) 0.5])
#
# ^Disabled: Incorrectly produces error when x = 0
#            Incorrectly produces negative numbers when acos does no such thing

#        asinh: log(x + sqrt(x*x + 1))
#        exp(asinh): x + sqrt(x*x + 1)#
# Disabled: On x86_64, "asinh(x)" is slower than "log(sqrt(x^2+1)+x)"
#cAdd {(cPow [(cAdd {(cPow [x 2])  1}) 0.5]) x} -> cPow [CONSTANT_E (cAsinh [x])]

#        acosh: log(x + sqrt(x*x - 1))
#        exp(acosh): x + sqrt(x*x - 1)
# Disabled: On x86_64, "acosh(x)" is slower than "log(sqrt(x^2-1)+x)"
#cLog  [(cAdd {(cPow [(cAdd {(cPow [x 2]) -1}) 0.5]) x})] -> cAcosh [x]
#cAdd {(cPow [(cAdd {(cPow [x 2]) -1}) 0.5]) x} -> cPow [CONSTANT_E (cAcosh [x])]

#        atanh(x):   log( (1+x) / (1-x)) / 2
#        2*atanh(x): log( (1+x) / (1-x))
cLog  [(cMul {(cAdd {1 x}) (cPow [(cAdd {1 (cMul {-1 x})}) -1])})] -> cMul [(cAtanh [x]) 2]

cAtan [(cMul {x (cPow [y -1])})] -> cAtan2 [x y]

#        asin(x): atan2(x, (1-x*x)^0.5)
#        asin(x): atan(x * (1-x*x)^-0.5)
cAtan2 [x (cPow [(cAdd {(cMul {(cPow [x 2]) -1}) 1}) 0.5])] -> cAsin[x]
cAtan [(cMul {(cPow [(cAdd {(cMul {(cPow [x 2]) -1}) 1}) -0.5]) x})] -> cAsin[x]

#        acos(x): atan2((1-x*x)^0.5, x)
#        acos(x): atan((1-x*x)^0.5 * x^-1)
cAtan2 [(cPow [(cAdd {(cMul {(cPow [x 2]) -1}) 1}) 0.5]) x] -> cAcos[x]


#        atanh(y*x):   log( (1+y*x) / (1+(-1*y*x))) / 2
#        2*atanh(y*x): log( (1+y*x) / (1+(-1*y*x)))

#        atanh(5*x):   log( (1+5*x) / (1+(-5*x))) / 2
#        2*atanh(5*x): log( (1+5*x) / (1+(-5*x)))

#        atanh(y+x):   log( (1+y+x) / (1+(-1*(y+x)))) / 2
#        2*atanh(y+x): log( (1+y+x) / (1+(-1*(y+x))))

#cLog [(cMul {(cAdd {(cMul {% x}) 1}) (cPow [(cAdd {(cMul {-% x}) 1}) -1])})] -> cMul [(cAtanh [(cMul % x)]) 2]

# atanh(x)              = log2( ((x*-2)+1) / ((x*2)-1) ) * log(2)/2
# atanh(x)*2/log(2)     = log2( ((x*-2)+1) / ((x*2)-1) )
# y^(atanh(x)*2/log(y)) = ((x*-y)+1) / ((x*y)-1)

#cMul (cAdd {(cMul {x %}) 1}) (cPow [(cAdd {(cMul {x -%}) -1}) -1]) : (cPow [% (cMul (cAtanh[x]) 2 /LOG(%))])
#cMul (cPow [(cAdd {(cMul {x %}) &}) -1]) (cAdd {(cMul {x -%}) 2 -&}) : (cPow [-% (cMul (cAtanh[(cAdd (cMul -% x) & -1)]) 2 /LOG(-%))])


#   tanh(x) / cosh(x)
# = sinh(x) / cosh(x)^2
# = sinh(x) * exp(x) / ((exp(x) * cosh(x))^2 * exp((-x)))
#
# ((y / ((exp(x) * z)^2 * exp((-x)))) * exp(x))  = y / z^2
# FIXME: Not detected yet



# cot(pi/2 - x) = 1/tan(pi/2 - x) = tan(x)
#                   tan(pi/2 - x) = 1/tan(x)
#                      reverse is probably better
#                      but cot() isn't exactly bad, so keep it
#cPow [(cTan[x]) -1] -> cTan [(cAdd [CONSTANT_PIHALF (cMul [-1 x])])]

#cMul (cTan [(cAdd {CONSTANT_PIHALF (cMul -1 <1>)})]) (cTan [(cMul <1>)]) : 1
#^ DISABLED: MULTIPLE/REPEATED RESTHOLDERS


# tan(atan(x)) = x
cTan [(cAtan [x])] -> x

cTan [(cAtan2 [x y])] -> cMul x (cPow [y -1])

[FINAL2]

# x * CONSTANT_DR = cDeg(x)
cMul  CONSTANT_DR <1> -> cDeg [(cMul <1>)]
# Note: This may produce one-operand cMul lists.
# Those are treated properly in the bytecode generation,
# and do not need to be handled by [INTERMEDIATE].

# x * CONSTANT_RD = cRad(x)
cMul  CONSTANT_RD <1> -> cRad [(cMul <1>)]

cFloor [(cAdd 0.5 <1>)] -> cInt [(cAdd <1>)]

# log(x) / CONSTANT_L10  = log10(x)
#cMul        (cLog [x])      CONSTANT_L10I  :        (cLog10 [x])
#cMul (cPow [(cLog [x]) -1]) CONSTANT_L10   : (cPow [(cLog10 [x]) -1])

# log(x) / CONSTANT_L2 = log2(x)
#cMul        (cLog [x])      CONSTANT_L2I  :        (cLog2 [x])
#cMul (cPow [(cLog [x]) -1]) CONSTANT_L2   : (cPow [(cLog2 [x]) -1])

# log2(x) * CONSTANT_L2 = log(x)
#cMul        (cLog2 [x])      CONSTANT_L2   :        (cLog [x])
#cMul (cPow [(cLog2 [x]) -1]) CONSTANT_L2I  : (cPow [(cLog [x]) -1])

# log2(x) * CONSTANT_L10B = log10(x)
#cMul        (cLog2 [x])      CONSTANT_L10B   :        (cLog10 [x])
#cMul (cPow [(cLog2 [x]) -1]) CONSTANT_L10BI  : (cPow [(cLog10 [x]) -1])

# log2(x) and log10(x) have turned out to be horrible at their performance
#   - except, log10 is faster on IA32, slower on X86_64
#
#cMul (cLog2  [x]) % : (cLog[x]) % CONSTANT_L2I
#cMul (cLog10 [x]) % : (cLog[x]) % CONSTANT_L10I
#cLog2  [x]         -> (cMul (cLog[x]) CONSTANT_L2I)
#cLog10 [x]         -> (cMul (cLog[x]) CONSTANT_L10I)
#cMul CONSTANT_L2  (cAdd { (cLog2[x]) % } )    : (cAdd (cLog [x]) LOG(POW(2 %)))
#cMul CONSTANT_L10 (cAdd { (cLog10[x]) % } )   : (cAdd (cLog [x]) LOG(POW(10 %)))

#cAdd (cMul {& (cLog2[x])}) *(LOG(&) CONSTANT_L2I) : cAdd (cLog2[x])  --unfinished

# inverted pow is better expressed with negated exponent
#  this makes x^-acos(x) , which is turned into 1/x^acos(x)
#  turn back into x^-acos(x)
# but do only if y is not integer constant -- TODO: Figure out why?

cPow [ (cSin[x]) %@N ] : (cCsc[x]) -%
cPow [ (cCos[x]) %@N ] : (cSec[x]) -%
cPow [ (cTan[x]) %@N ] : (cCot[x]) -%
#cPow [ (cCsc[x]) %@F@N ] : (cSin[x]) -%
#cPow [ (cSec[x]) %@F@N ] : (cCos[x]) -%
#cPow [ (cCot[x]) %@F@N ] : (cTan[x]) -%

# These optimizations are fine when used as truth values, but because
# they produce values other than 0 or 1, they cannot really be used.
#cOr  x@L y@L          : (cAdd x y)
#cOr  x@L (cAdd <1>)@P : (cAdd x <1>)
#cAnd x@L          y   : (cMul x y)

cNot    [x@P] -> cAbsNot [x]
cNotNot [x@P] -> cAbsNotNot [x]
cAnd x@P y@P <1> -> cAbsAnd x y (cAnd <1>)
cOr x@P y@P <1>  -> cAbsOr x y (cOr <1>)
cIf [x@P y z]    -> cAbsIf x y z
cAbsAnd 1 :
cAbsOr  0 :
