# This datafile documents all the possible optimizations that the optimizer can/should do.
# It is parsed by the parser described in fpoptimizer_grammar_gen.y, which
# is compiled into C++ code in fpoptimizer_grammar_gen.cc. The parser produces
# a C++ file, fpoptimizer_grammar.cc , which lists the grammar rules in tabular
# format. The grammar rules are utilized by fpoptimizer_optimize.cc , which
# matches the function trees into the rules and performing those replacements
# which can be performed.
#
# Copyright 2009 Joel Yliluoma, written specifically
#               for Warp's Function Parser (fparser).
#

# Substitution rule syntax:
#
# %token NUMERIC_CONSTANT     # literals such as 0, 1, 1.5 or CONSTANT_DR, CONSTANT_L10I
# %token PARAMETER_TOKEN      # placeholders such as x, y, a, b
# %token POSITIVE_PARAM_TOKEN # placeholders p and q, which match POSITIVE params only
# %token NEGATIVE_PARAM_TOKEN # placeholders m and n, which match NEGATIVE params only
# %token EVEN_PARAM_TOKEN     # placeholders e and f, which match EVEN params only
# %token NONEVEN_PARAM_TOKEN  # placeholders g and h, which match anything but EVEN params
# %token ODD_PARAM_TOKEN      # placeholders o and r, which match ODD params only
# %token PLACEHOLDER_TOKEN    # placeholders such as <1>, <2>, <7>
# %token IMMED_TOKEN          # placeholders % and &
# %token NEGATIVE_IMMED_TOKEN # placeholder $
# %token BUILTIN_FUNC_NAME    # such as COS, CEIL, POW, +, *, MIN, MAX
# %token OPCODE               # opcodes
# %token UNARY_TRANSFORMATION  # /, -, !  # inverts/negates/inverts the param
# %token NEWLINE              # newline
#
# %token SUBST_OP_COLON       # :
# %token SUBST_OP_ARROW       # ->
#
# %%
#     grammar:
#       grammar substitution
#     | grammar NEWLINE
#     | /* empty */
#     ;
#
#     substitution:
#       function SUBST_OP_ARROW param NEWLINE
#       /* Entire function is changed into the particular param */
#
#     | function SUBST_OP_ARROW function NEWLINE
#       /* Entire function changes, the param_notinv_list is rewritten */
#       /* NOTE: "p x -> o y"  is a shortcut for "p x -> (o y)"  */
#
#     | function SUBST_OP_COLON  paramlist NEWLINE
#       /* The params provided are replaced with the new param_maybeinv_list */
#     ;
#
#     function:
#        OPCODE '[' paramlist sign_spec ']'
#        /* Match a function with opcode=opcode,
#         * and the exact parameter list as specified
#         */
#        OPCODE '{' paramlist sign_spec '}'
#        /* Match a function with opcode=opcode,
#         * and the exact parameter list in any order
#         */
#     |  OPCODE paramlist sign_spec
#        /* Match a function with opcode=opcode and the given way of matching params */
#        /* There may be more parameters, don't care about them */
#     ;
#
#     paramlist: /* left-recursive list of 0-n params with no delimiter */
#         paramlist '~' param /* negated/inverted param */
#       | paramlist param
#       | /* empty */
#     ;
#
#     sign_spec: /* Can be used to specify a proportion of negativeness to positiveness */
#         '=-' /* ~<number> matched more elements than <number> did */
#       | '=+' /* ~<number> matched fewer elements than <number> did */
#       | '==' /* ~<number> matched as many elements than <number> did */
#       | /* empty */
#     ;
#
#     param:
#        NUMERIC_CONSTANT         /* particular immed */
#     |  IMMED_TOKEN              /* a placeholder for some immed */
#     |  NEGATIVE_IMMED_TOKEN     /* a placeholder for a negative immed */
#     |  BUILTIN_FUNC_NAME '(' paramlist ')'  /* literal logarithm/sin/etc. of the provided immed-type params -- also sum/product/minimum/maximum */
#     |  UNARY_TRANSFORMATION param   /* the negated/inverted literal value of the param */
#     |  expression_param         /* expression param */
#     |  expression_param '+'      /* expression param, repeated 2..N times. */
#     |  expression_param '*'      /* expression param, repeated 1..N times. */
#     |  '(' function ')'         /* a subtree */
#     |  PLACEHOLDER_TOKEN        /* a placeholder for all params */
#     ;
#
#     expression_param:
#        PARAMETER_TOKEN          /* any expression, indicated by "x", "a" etc. */
#      | POSITIVE_PARAM_TOKEN
#      | NEGATIVE_PARAM_TOKEN
#      | EVEN_PARAM_TOKEN
#      | NONEVEN_PARAM_TOKEN
#      | ODD_PARAM_TOKEN
#     ;


[BASIC]

###### Note: Before adding new rules (especially those which handle constant values),
######       verify that it is not already done in ConstantFolding().

##### TODO:
#####    Figure out whether it would be easier to produce optimization rules
#####    if our Add lists never had polarity (instead, a cMul with -1 would be used)
#####    and    Mul lists never had polarity (instead, a cPow with -1 would be used)

#### Remove redundant components:

# In a add-list, having x and -x gets both deleted
#cAdd x (cMul {x -1})  :
# In a mul-list,  having x and 1/x gets both deleted
#cMul x (cPow [x -1])  :
# In a min-list, two identical components are reduced into one
cMin x x    : x
# In a max-list, two identical components are reduced into one
cMax x x    : x

# If both branches of an If() are identical, the test becomes unnecessary
cIf [y x x] -> x
# If the If() condition begins with a not, remove the not and swap the branches
cIf [(cNot x) y z]    -> cIf [x z y]
# Notnots are unnecessary in an If()
cIf [(cNotNot x) y z] -> cIf [x y z]

# There are more optimizations that could be done with If(),
# but they are likely too specific.
# For example:
#  Move branch invariants. if(y, x+a, x+b) -> x+if(y, a, b)
#  TODO: Figure out why these rules never match.
cIf [y       x      (cAdd x <1>)] -> cAdd[x (cIf [y 0          (cAdd <1>)])]
cIf [y (cAdd x <1>) (cAdd x <3>)] -> cAdd[x (cIf [y (cAdd <1>) (cAdd <3>)])]
cIf [y       x      (cMul x <1>)] -> cMul[x (cIf [y 1          (cMul <1>)])]
cIf [y (cMul x <1>) (cMul x <3>)] -> cMul[x (cIf [y (cMul <1>) (cMul <3>)])]
cIf [y       x      (cAnd x <1>)] -> cAnd[x (cIf [y 0          (cAnd <1>)])]
cIf [y (cAnd x <1>) (cAnd x <3>)] -> cAnd[x (cIf [y (cAnd <1>) (cAnd <3>)])]
cIf [y       x      (cOr  x <1>)] -> cOr [x (cIf [y 1          (cOr  <1>)])]
cIf [y (cOr  x <1>) (cOr  x <3>)] -> cOr [x (cIf [y (cOr  <1>) (cOr  <3>)])]

#### Flattening the topology of add/mul/min/max/and/or groups:

## If an add-list contains a mul-list with a negated entry, assimilate the negation.
## I.e.  a + b + (c * (-d-e+f))  ->  a + b - (c * (+d+e-f))
## Or    a + b - (c * (-d-e+f))  ->  a + b + (c * (+d+e-f))
#cAdd  (cMul (cAdd <3> ~<4> =-) <1> ~<2>) : ~(cMul (cAdd ~<3> <4>) <1> ~<2>)
#cAdd ~(cMul (cAdd <3> ~<4> =-) <1> ~<2>) :  (cMul (cAdd ~<3> <4>) <1> ~<2>)

# If an and-list contains not-tokens, assimilate those not-tokens
cAnd  (cNot[x])   : ~x
cAnd ~(cNot[x])   : x

# A notnot in and-lists is redundant:
cAnd  (cNotNot[x]) :  x
cAnd ~(cNotNot[x]) : ~x

# If an or-list contains not-tokens, assimilate those not-tokens
cOr  (cNot[x])   : ~x
cOr ~(cNot[x])   : x

# A notnot in or-lists is redundant:
cOr   (cNotNot[x]) :  x
cOr  ~(cNotNot[x]) : ~x

# If an and-list contains sign-flipped and-list, assimilate as inverted or list
cAnd ~(cAnd <1> ~<2>)  : (cOr ~<1> <2>)

# If an or-list contains sign-flipped or-list, assimilate as inverted and list
cOr ~(cOr <1> ~<2>)   : (cAnd ~<1> <2>)

# Two notnots make one
cNotNot [(cNotNot[x])] : x
# A notnot-not is better expressed as not-notnot
cNotNot [(cNot[x])] -> cNot [(cNotNot [x])]

#### Logarithm optimizations
# log(x^y) = y*log(x)
#cLog [(cPow [p y])] -> cMul [y (cLog[p])]
cLog2 [(cPow [p y])] -> cMul [y (cLog2[p])]

# log(x^e) = e * log(abs(x))   (e is an even integer)
#cLog [(cPow [x e])] -> cMul [e (cLog [(cAbs [x])])]
cLog2 [(cPow [x e])] -> cMul [e (cLog2 [(cAbs [x])])]

# CONSTANT_E^log(x) = x
#cPow [CONSTANT_E (cLog[x])]   -> x
#cPow [2          (cLog2[x])]   -> x

# Generalized as:  y^log(x) = x^log(y)
#cPow [% (cLog[x])]  : x LOG(%)
cPow [% (cLog2[x])] : x LOG2(%)

# Because log(exp(6)*x) = log(x)+6, we can also do this:
#                  y^(log(x)+z)
#                = y^(log(x*exp(z)))
#                = (x*exp(z))^log(y)
#cPow [y (cAdd {(cLog[x]) %})] : (cMul x EXP(%)) (cLog y)
# Probably beneficial to do it only when y is const,
# though. Otherwise we only trade + for *, which is bad.
#cPow [& (cAdd {(cLog[x]) %})] : (cMul x EXP(%)) LOG(&)
cPow [& (cAdd {(cLog2[x]) %})] : (cMul x POW(2 %)) LOG2(&)

# CONSTANT_E^(log(x)*y) = x^y
#cPow [CONSTANT_E (cMul (cLog[x]) <1>)]   :     x  (cMul <1>)
cPow [2          (cMul (cLog2[x]) <1>)]  :     x  (cMul <1>)

# Generalization of the above.
#  Note: if you have exp(log(x)*5), you may get two results (ambiguous rules):
#                   a: x^5            -- the rule above,
#                   b: exp(5)^log(x)  -- the rule below.
#  Naturally, case "a" is more desirable,
#  but if you get "b", the y^log(x) = x^log(y) rule will fix it so it becomes x^5.
cPow [%          (cMul & <1>)]   : POW(% &) (cMul <1>)

# z^(log(x)/log(z)*y) = x^y
#cPow [z (cMul (cPow [(cLog[z])  -1]) (cLog[x])  <1>)] : x (cMul <1>)
cPow [z (cMul (cPow [(cLog2[z]) -1]) (cLog2[x]) <1>)] : x (cMul <1>)
#cPow [% (cMul /LOG(%)               (cLog[x])  <1>)] : x (cMul <1>)
cPow [% (cMul /LOG2(%)              (cLog2[x]) <1>)] : x (cMul <1>)

# log(x) + log(y) = log(x*y)
#cAdd (cLog[x])  (cLog[y])  : (cLog  (cMul [x y]))
cAdd (cLog2[x]) (cLog2[y]) : (cLog2 (cMul [x y]))
# When x is const, the reverse is more beneficial
#  i.e.  log(2*x) = log(x) + log(2)
#cLog  [(cMul % <1>)] -> cAdd (cLog  [(cMul <1>)]) LOG(%)
cLog2 [(cMul % <1>)] -> cAdd (cLog2 [(cMul <1>)]) LOG2(%)

# log(x * z^y) = (log(x) / log(z) + y) * log(z)
# Only worthwhile when z is an immed
# Note that when z = CONSTANT_E, this reduces rather nicely into log(x) + y
#cLog  [(cMul (cPow [% y]) <1>)] -> cMul [LOG(%) (cAdd [y (cMul (cLog [(cMul <1>)]) /LOG(%))])]
cLog2 [(cMul (cPow [% y]) <1>)] -> cMul [LOG2(%) (cAdd [y (cMul (cLog2 [(cMul <1>)]) /LOG2(%))])]
# When y=1, the reverse is more useful:
#cMul {% (cAdd {1 (cMul {(cLog  [x]) /%})})} -> cAdd (cLog  [x]) %
cMul {% (cAdd {1 (cMul {(cLog2 [x]) /%})})} -> cAdd (cLog2 [x]) %


#### Linear combining and polynomials

# If a mul-list contains the same element two or more times,
# replace the element with a pow with an integer exponent
# (note: inversions are treated as a negative exponent)
# e.g. ...*x*x*x*x = ...*x^4
cMul  x+                        : (cPow  x  x+)
cAdd  x+                        : (cMul  x   x+)
# x^3 * x = x^4
cMul  x*           (cPow[x y])  : (cPow [x (cAdd x* y)])
cAdd  x*           (cMul x <1>) : (cMul [x (cAdd x* (cMul <1>) )])
# x^y * x^z = x^(y+z)
cMul  (cPow[x y])  (cPow[x z]) : (cPow [x (cAdd [y z])])

#### Remote double negations/inversions

# If a pow has an exponent of 1, replace the pow with the base value
cPow [x 1]  -> x

# (x^g)^z        -> x^(g*z)  (g is not an even integer)
# FIXME: Should this be only done for odd integers?
# Because "not even" _can_ be even, we just don't know for sure
cPow [ (cPow[x g]) z ]       : x (cMul [g z])
# (x^e)^z        -> abs(x)^(e*z)  (e is an even integer)
cPow [ (cPow[x e]) z ]       : (cAbs [x]) (cMul [e z])

cPow [(cPow [p y]) z]        : p (cMul [y z])

#### Logical optimizations

# Repetitions in and-lists don't help
cAnd  x x   : x
cAnd ~x ~x  : ~x
# Nor in or-lists
cOr   x  x  : x
cOr  ~x ~x  : ~x

# In an and-list, having x and !x invalides the whole list
cAnd x ~x   -> 0

# In an or-list, having x and !x validates the whole list
cOr x ~x    -> 1

#   x==x -- this statement is always true (note: might be not when we have NaN, but we don't care)
cEqual[x x]                     -> 1
cLessOrEq[x x]                  -> 1
cGreaterOrEq[x x]               -> 1

#   x!=x -- this statement is always false (note: we ignore NaN cases)
cNEqual[x x]                    -> 0
cLess[x x]                      -> 0
cGreater[x x]                   -> 0

# Eschew so many negations.
#   !a & !b = !(a | b)
cAnd <1> ~<2> =- : ~(cOr <2> ~<1>)
#   !a | !b = !(a & b)
cOr <1> ~<2> =- : ~(cAnd <2> ~<1>)

cAnd ~(cNEqual[a b])      : (cEqual[a b])
cAnd ~(cEqual[a b])       : (cNEqual[a b])
cAnd ~(cLess[a b])        : (cGreaterOrEq[a b])
cAnd ~(cGreater[a b])     : (cLessOrEq[a b])
cAnd ~(cLessOrEq[a b])    : (cGreater[a b])
cAnd ~(cGreaterOrEq[a b]) : (cLess[a b])
cOr ~(cNEqual[a b])       : (cEqual[a b])
cOr ~(cEqual[a b])        : (cNEqual[a b])
cOr ~(cLess[a b])         : (cGreaterOrEq[a b])
cOr ~(cGreater[a b])      : (cLessOrEq[a b])
cOr ~(cLessOrEq[a b])     : (cGreater[a b])
cOr ~(cGreaterOrEq[a b])  : (cLess[a b])
cNot [(cNEqual[a b])]      -> cEqual[a b]
cNot [(cEqual[a b])]       -> cNEqual[a b]
cNot [(cLess[a b])]        -> cGreaterOrEq[a b]
cNot [(cGreater[a b])]     -> cLessOrEq[a b]
cNot [(cLessOrEq[a b])]    -> cGreater[a b]
cNot [(cGreaterOrEq[a b])] -> cLess[a b]
cNot [(cAnd <1> ~<2>)]     -> cOr  ~<1> <2>
cNot [(cOr  <1> ~<2>)]     -> cAnd ~<1> <2>
cNotNot [(cNEqual[a b])]      -> cNEqual[a b]
cNotNot [(cEqual[a b])]       -> cEqual[a b]
cNotNot [(cLess[a b])]        -> cLess[a b]
cNotNot [(cGreater[a b])]     -> cGreater[a b]
cNotNot [(cLessOrEq[a b])]    -> cLessOrEq[a b]
cNotNot [(cGreaterOrEq[a b])] -> cGreaterOrEq[a b]
cNotNot [(cAnd <1> ~<2>)]     -> cAnd <1> ~<2>
cNotNot [(cOr  <1> ~<2>)]     -> cOr  <1> ~<2>

# From logic, follows that...
#   (a==b) & (b==c) & (a==c) -- one of these is redundant
cAnd (cEqual[a b]) (cEqual[b c]) (cEqual[a c])  : (cEqual[a b]) (cEqual[b c])
#   (a==b) & (a!=b)          -- this statement is always false
cAnd (cEqual[a b]) (cNEqual[a b]) -> 0
#   (a==b) | (a!=b)          -- this statement is always true
cOr  (cEqual[a b]) (cNEqual[a b]) -> 1

cAnd (cLess[a b]) (cGreater[a b]) -> 0
cAnd (cLessOrEq[a b]) (cGreater[a b]) -> 0
cAnd (cLess[a b]) (cGreaterOrEq[a b]) -> 0
cOr (cLess[a b]) (cGreater[a b]) -> 1
cOr (cLessOrEq[a b]) (cGreater[a b]) -> 1
cOr (cLess[a b]) (cGreaterOrEq[a b]) -> 1






[ENTRY]
# These optimizations are done only once, before the intermediate conversions.
# This process converts some optimized opcodes into primitives, so as to
# make the intermediate optimizations simpler to write. In the final stage,
# the optimized opcodes are reconstructed where available.

# not(not(x)) = notnot(x)
# Do this so that !!x won't get inadvertedly optimized into x
# However, optimizing !!!x into !x is allright, and we do quite
# many things relating to cNot, so cNotNot helps protecting the
# one special case without having to repeat protections everywhere.
cNot[(cNot [x])] -> cNotNot [x]

#        asinh: log(x + sqrt(x*x + 1))
cAsinh [x] -> cMul (cLog2 (cAdd x (cPow (cAdd (cPow x 2) 1) 0.5))) CONSTANT_L2

#        acosh: log(x + sqrt(x*x - 1))
cAcosh [x] -> cMul (cLog2 (cAdd x (cPow (cAdd (cPow x 2) -1) 0.5))) CONSTANT_L2

#        atanh: log( (1+x) / (1-x)) / 2
cAtanh [x] -> cMul (cLog2 (cMul (cAdd 1 x) (cPow (cAdd 1 (cMul -1 x)) -1))) *(0.5 CONSTANT_L2)

#     The hyperbolic functions themselves are:
#        sinh: (exp(x)-exp(-x)) / 2
cSinh [x] -> cMul 0.5 (cAdd (cPow [CONSTANT_E x]) (cMul [-1 (cPow [CONSTANT_EI x])]))

#        cosh: (exp(x)+exp(-x)) / 2
#        cosh(-x) = cosh(x)
cCosh [x] -> cMul 0.5 (cAdd (cPow [CONSTANT_E x]) (cMul [   (cPow [CONSTANT_EI x])]))

#        tanh: sinh/cosh = (exp(2*x)-1) / (exp(2*x)+1)
cTanh [x] -> (cMul (cAdd {(cPow [CONSTANT_2E x]) -1}) (cPow [(cAdd {(cPow [CONSTANT_2E x]) 1}) -1]))

#        tan: sin/cos
cTan [x] -> (cMul (cSin [x]) (cPow [(cCos [x]) -1]))

# Should we change sin(x) into cos(pi/2-x)
#               or cos(x) into sin(pi/2-x)?
#                        note: cos(x-pi/2) = cos(pi/2-x) = sin(x)
#                        note: sin(x-pi/2) = -sin(pi/2-x) = -cos(x)

#cLog [x] -> (cMul [CONSTANT_L2 (cLog2 [x]) ])

[INTERMEDIATE]

# Opcodes we will NOT find in the intermediate stage:
#  Done by bytecode parser:
#   Meta opcodes: cDup, cNop, cFetch, cPopNMov, cJump
#   Meta opcodes: cVar, cImmed
#   Implemented through cMul: cDiv, cRDiv, cInv, cSqr
#   Implemented through cAdd: cSub, cRSub, cNeg
#   Implemented through constant-multiplying: cDeg, cRad
#   Implemented through cSin, cCos: cCot, cCsc, cSec, cTan
#   Implemented through cPow: cSqrt, cExp
#   Implemented through cLog2: cLog, cLog10
#  Done by entry rules:
#   Extracted: cAsinh, cAcosh, cAtanh
#   Extracted: cSinh, cCosh, cTanh

#### CONTINUED: Flattening the topology of add/mul/min/max/and/or groups

# a^2 + a*b*X/Z + b^2 = (a+b)^2 + (X/Z-2)*(a*b)
cAdd (cPow[a 2]) (cPow[b 2]) (cMul a b <1>) : (cPow [(cAdd [a b]) 2]) (cMul [a b (cAdd [(cMul <1>) -2])])
# For optimizing x^2+2*x*y+y^2:
#  With this rule,    eval=0.287154 us, optimized = 0.0758879 us
#  Without this rule, eval=0.314538 us, optimized = 0.0831386 us
# For optimizing x^2+3*x*y+y^2:
#  With this rule,    eval=0.295956 us, optimized = 0.0781288 us
#  Without this rule, eval=0.300723 us, optimized = 0.075689 us
# The benchmark results seem too varying, so it is hard to tell
# whether this rule had some advantage. It _looks_ like it did
# though, so better keep it, I suppose. -Bisqwit
#
# How about this?
# (a+b+c)^2 = c^2 + 2*b*c + 2*a*c + b^2 + 2*a*b + a^2
# Seems that it becomes:
# a^2 + b^2 + c^2 + 2*((a+b)*c + a*b)
# Is it worth adding rule for making that into (a+b+c)^2?
# Too specific, I suppose.



cAdd (cMul <1> <3>)  (cMul <1> <5>) : (cMul <1> (cAdd (cMul <3>)  (cMul <5>)))
# Note: The above line can require extremely time-consuming

# These are the same as above, but work also if pow() is expanded
# Note: It would work even with y and z instead of % and &, but we
# limit into numeric literals for simplicity.
cAdd (cMul (cPow[x %]) <3>)  (cMul (cPow[x &]) <5>) : (cMul (cPow[x MIN(% &)]) (cAdd (cMul <3> (cPow[x (cAdd % -MIN(% &))]))  (cMul <5> (cPow[x (cAdd & -MIN(% &))]))))
cAdd (cMul (cPow[x %]) <3>)  (cMul x           <5>) : (cMul (cPow[x MIN(% 1)]) (cAdd (cMul <3> (cPow[x (cAdd % -MIN(% 1))]))  (cMul <5> (cPow[x (cAdd 1 -MIN(% 1))]))))

#    (5.1*x +      4.1*y      + z+w)*2
# -> (5.1*2*x + 2*(4.1*y      + z+w))
# -> (5.1*2*x +   (4.1*2*y + 2*(z+w)))
cMul (cAdd (cMul % <1>) <3>) &  :  (cAdd (cMul *(% &) <1>) (cMul & (cAdd <3>)))

#    (2+x+y)*4 = 2*4 + 4*(x+y)
#cMul (cAdd % <1>) &  :  (cAdd *(% &) (cMul & (cAdd <1>)))
#
# ^ Good idea, but conflicts with cAdd x* (cMul x <1>)
#   When given 2*x+2
#     You get 2*(1+x)
#     And     2+ 2*x
#     Alternatingly.
#     To fix, should only be done when % != 1.
#     Unfortunately there's no syntax for that kind of constraints...
#

#### Constant folding is now performed exclusively by ConstantFolding()

#### Trigonometric:
# sin(-x) = -sin(x)
cSin [(cMul $ <1>)] -> cMul -1 (cSin [(cMul -$ <1>)])
# cos(-x) = cos(x)
cCos [(cMul $ <1>)] : (cMul -$ <1>)


# cos(pi/2 - x) = sin(x)
cCos [(cAdd {CONSTANT_PIHALF (cMul $ <1>)})] -> cSin[(cMul -$ <1>)]
# sin(pi/2 - x) = cos(x)
cSin [(cAdd {CONSTANT_PIHALF (cMul $ <1>)})] -> cCos[(cMul -$ <1>)]
# cos(x - pi/2) = cos(pi/2 - x) = sin(x)
cCos [(cAdd -CONSTANT_PIHALF <1>)]            -> cSin[(cAdd <1>)]
# sin(x - pi/2) = -sin(pi/2 - x) = -cos(x)
cSin [(cAdd -CONSTANT_PIHALF <1>)]            -> cMul -1 (cCos[(cAdd <1>)])

# sin(x)^2 + cos(x)^2 = 1
cAdd  (cPow[ (cSin[x]) 2]) (cPow [(cCos[x]) 2]) : 1
# y-sin(x)^2 = cos(x)^2+(y-1)
# y-cos(x)^2 = sin(x)^2+(y-1)
cAdd 1 (cMul { -1 (cPow[ (cSin[x]) 2]) }) : (cPow [(cCos[x]) 2])
cAdd 1 (cMul { -1 (cPow[ (cCos[x]) 2]) }) : (cPow [(cSin[x]) 2])

# sin(x)*cos(y) + cos(x)*sin(y) = sin(x+y)
# sin(x)*cos(y) - cos(x)*sin(y) = sin(x-y)
# cos(x)*cos(y) + sin(x)*sin(y) = cos(x+y)
# cos(x)*cos(y) - sin(x)*sin(y) = cos(x-y)

cAdd  (cMul {(cSin[x]) (cCos[y])}) (cMul {(cCos[x]) (cSin[y])   }) :  (cSin [(cAdd[x           y]  )])
cAdd  (cMul {(cSin[x]) (cCos[y])}) (cMul {(cCos[x]) (cSin[y]) -1}) :  (cSin [(cAdd[x (cMul [-1 y])])])
cAdd  (cMul {(cCos[x]) (cCos[y])}) (cMul {(cSin[x]) (cSin[y])   }) :  (cCos [(cAdd[x           y]  )])
cAdd  (cMul {(cCos[x]) (cCos[y])}) (cMul {(cSin[x]) (cSin[y]) -1}) :  (cCos [(cAdd[x (cMul [-1 y])])])

cAdd  (cMul {(cSin[x]) (cCos[y]) -1}) (cMul {(cCos[x]) (cSin[y]) -1}) : (cMul [-1 (cSin [(cAdd[x           y]  )]) ])
cAdd  (cMul {(cCos[x]) (cCos[y]) -1}) (cMul {(cSin[x]) (cSin[y]) -1}) : (cMul [-1 (cCos [(cAdd[x           y]  )]) ])
cAdd  (cMul {(cCos[x]) (cCos[y]) -1}) (cMul {(cSin[x]) (cSin[y])   }) : (cMul [-1 (cCos [(cAdd[x (cMul [-1 y])])]) ])
#cAdd (cMul {(cSin[x]) (cCos[y]) -1}) (cMul {(cCos[x]) (cSin[y])   }) : (cMul [-1 (cSin [(cAdd[x (cMul [-1 y])])]) ])
# ^This one is redudant: It just reaffirms that sin(x) = -sin(-x).


#### Self-defeating function calls:

# sin(asin(x)) = x
cSin [(cAsin [x])] -> x

# cos(acos(x)) = x
cCos [(cAcos [x])] -> x

# Note: asin(sin(x)) must not be converted, because
# even though asin(sin(1.1)) = 1.1, asin(sin(1500)) != 1500.

cAbs [(cMul n <1>)] -> cMul (cAbs [n]) <1>

# abs(x)^e -> x^e when e=even integer
cPow [(cAbs[x]) e] : x e



[FINAL1]
# These optimizations are done only once, after the intermediate conversions.
# This process generates high-level opcodes that are not
# expected to be found in the intermediate stage.
#
# Do not generate the following opcodes here:
#     cDiv, cRDiv, cInv, cSub, cRSub, cNeg, cNot
#     cSqrt, cRSqrt, cExp
#     cCsc, cSec, cCot
# Do not reduce add/mul/min/max/and/or lists to two-operand topology.
# Those are done in the bytecode generation automatically.
#

cAtan2 [(cMul  x <1>) (cMul  x <3>)]   : (cMul <1>) (cMul <3>)
#cAtan2 [(cMul  % <1>) (cMul  % <3>)]   : (cMul <1>) (cMul <3>)

# sinh(x)/cosh(x) = tanh(x)
cMul        (cSinh[x])     (cPow [(cCosh[x]) -1]) :        (cTanh[x])
cMul (cPow [(cSinh[x]) -1])       (cCosh[x])      : (cPow [(cTanh[x]) -1])
cMul        (cTanh[x])            (cCosh[x])      :        (cSinh[x])
cMul        (cTanh[x])     (cPow [(cSinh[x]) -1]) : (cPow [(cCosh[x]) -1])
cMul (cPow [(cTanh[x]) -1])       (cSinh[x])      :        (cCosh[x])

# sin(x)/cos(x) = tan(x)
cMul        (cSin[x])     (cPow [(cCos[x]) -1]) :        (cTan[x])
cMul (cPow [(cSin[x]) -1])       (cCos[x])      : (cPow [(cTan[x]) -1])
cMul        (cTan[x])            (cCos[x])      :        (cSin[x])
cMul        (cTan[x])     (cPow [(cSin[x]) -1]) : (cPow [(cCos[x]) -1])
cMul (cPow [(cTan[x]) -1])       (cSin[x])      :        (cCos[x])

# sinh(-x) = -sinh(x)
cSinh [(cMul $ <1>)] -> cMul [-1 (cSinh [(cMul -$ <1>)])]

# cosh(-x) = cosh(x)
cCosh [(cMul $ <1>)] : (cMul -$ <1>)

# tan(-x) = -tan(x)
cTan [(cMul $ <1>)] -> cMul [-1 (cTan [(cMul -$ <1>)])]

# tanh(-x) = -tanh(x)
cTanh [(cMul $ <1>)] -> cMul [-1 (cTanh [(cMul -$ <1>)])]


# sinh(x)*2 = (exp(x)-   exp(-x))
# sinh(x)*2 = (exp(x)- 1/exp(x))
# cosh(x)*2 = (exp(x)+   exp(-x))
# cosh(x)*2 = (exp(x)+ 1/exp(x))
cAdd (cPow [&  x]) (cMul { -1 (cPow [/& x]) }) : (cMul (cSinh [(cMul x LOG(&))]) 2)
cAdd (cPow [&  x])            (cPow [/& x])    : (cMul (cCosh [(cMul x LOG(&))]) 2)

# Because sinh(-x) = -sinh(x),
# sinh(x)*-2 = (exp(-x)-exp(x))
cAdd (cMul {-1 (cPow [& x])})            (cPow [/& x])   : (cMul (cSinh [(cMul x LOG(&))]) -2)
cAdd (cMul {% (cPow [& x])})  (cMul { -% (cPow [/& x])}) : (cMul (cSinh [(cMul x LOG(&))]) *( 2 %))
cAdd (cMul {% (cPow [& x])})  (cMul {  % (cPow [/& x])}) : (cMul (cCosh [(cMul x LOG(&))]) *( 2 %))

#        tanh(x) = (exp(2*x)-1) / (exp(2*x)+1)
#      1/tanh(x) = (exp(2*x)+1) / (exp(2*x)-1)
#        tanh(-x) = -tanh(x), so
#       -tanh(x) = (exp(-2*x)-1) / (exp(-2*x)+1)
#     1/-tanh(x) = (exp(-2*x)+1) / (exp(-2*x)-1)
cMul (cAdd {(cPow [CONSTANT_2EI x]) -1}) (cPow [(cAdd {(cPow [CONSTANT_2EI x])  1}) -1]) : -1 (cTanh [x])
cMul (cAdd {(cPow [CONSTANT_2E  x]) -1}) (cPow [(cAdd {(cPow [CONSTANT_2E  x])  1}) -1]) : (cTanh [x])
cMul (cAdd {(cPow [CONSTANT_2EI x])  1}) (cPow [(cAdd {(cPow [CONSTANT_2EI x]) -1}) -1]) : (cPow [(cMul [-1 (cTanh [x])]) -1])
cMul (cAdd {(cPow [CONSTANT_2E  x])  1}) (cPow [(cAdd {(cPow [CONSTANT_2E  x]) -1}) -1]) : (cPow [(cTanh [x]) -1])

# Note: I have no idea why (exp(x)-exp(-x)) / (exp(x)+exp(-x))
#                 produces (exp(2*x)-1) / (exp(2*x)+1)
#       It just says so in Wikipedia.
#       Maybe this could be utilized for more generic optimizations?
#
# Maxima gives that exp(x) + n*exp(-x)
#                 = exp(-x) * (exp(2*x) + n)
# but I still have no idea why is that.
#
# In more generic form:
#   exp( d*y) * ((a * exp(b*x + -1*d*y)) + c)   = a*exp(    b*x) + c*exp( d*y)
#
#   exp(   y) * ((a * exp(  x + -1*  y)) + c)   = a*exp(      x) + c*exp(   y)
#   exp(   y) * ((    exp(  x + -1*  y)) + c)   =   exp(      x) + c*exp(   y)
#
#   exp( d*x) * ((a * exp(x*(b + -1*d))) + c)   = a*exp(    b*x) + c*exp( d*x)
#   exp(   x) * ((a * exp(x*b         )) + c)   = a*exp((b+1)*x) + c*exp(   x)
#   exp(   x) * ((a * exp(x           )) + c)   = a*exp(    2*x) + c*exp(   x)
# This is quite hairy.
# A rare special case too.
#
#cMul (cPow [CONSTANT_E y]) (cAdd <2> (cMul <1> (cPow [CONSTANT_E (cAdd <3> (cMul {-1 y}))]))) : (cAdd (cMul (cAdd <2>) (cPow [CONSTANT_E y])) (cMul <1> (cPow [CONSTANT_E (cAdd <3>)])))
#cMul (cPow [CONSTANT_E y]) (cAdd <2>           (cPow [CONSTANT_E (cAdd <3> (cMul {-1 y}))]))  : (cAdd (cMul (cAdd <2>) (cPow [CONSTANT_E y]))           (cPow [CONSTANT_E (cAdd <3>)]))
#cMul (cPow [CONSTANT_E x]) (cAdd <2> (cMul <1> (cPow [CONSTANT_E x]))) : (cAdd (cMul (cAdd <2>) (cPow [CONSTANT_E x])) (cMul <1> (cPow [CONSTANT_E (cMul [2 x])])))
#cMul (cPow [CONSTANT_E x]) (cAdd <2> (cMul <1> (cPow [CONSTANT_E (cMul x <3>)]))) : (cAdd (cMul (cAdd <2>) (cPow [CONSTANT_E x])) (cMul <1> (cPow [CONSTANT_E  (cMul [(cAdd 1 (cMul <3>)) x])])))


# exp(x)  = cosh(x)+sinh(x)
# exp(-x) = cosh(x)-sinh(x)
cAdd           (cCosh [x])             (cSinh [x])   :           (cPow [CONSTANT_E         x])
cAdd           (cCosh [x])   (cMul {-1 (cSinh [x])}) :           (cPow [CONSTANT_EI        x])
cAdd (cMul {-1 (cCosh [x])}) (cMul {-1 (cSinh [x])}) : (cMul [-1 (cPow [CONSTANT_E         x])])
cAdd (cMul {-1 (cCosh [x])})           (cSinh [x])   : (cMul [-1 (cPow [CONSTANT_EI        x])])
cAdd           (cCosh [x])   (cMul {-1 (cPow [CONSTANT_E  x])}) :           (cSinh [x])
cAdd           (cSinh [x])   (cMul {-1 (cPow [CONSTANT_E  x])}) :           (cCosh [x])
cAdd (cMul {-1 (cCosh [x])})           (cPow [CONSTANT_E  x])   : (cMul [-1 (cSinh [x])])
cAdd (cMul {-1 (cSinh [x])})           (cPow [CONSTANT_E  x])   : (cMul [-1 (cCosh [x])])

# sinh(acosh(x)) = sqrt(x^2 - 1)  (not a typo)
# cosh(asinh(x)) = sqrt(x^2 + 1)  (not a typo)
#  Not sure whether these are faster. They are more opcodes, but
#  simpler. The rationale is in allowing for further optimizations.
cSinh [(cAcosh [x])] -> cPow [(cAdd [(cPow [x 2]) -1]) 0.5]
cCosh [(cAsinh [x])] -> cPow [(cAdd [(cPow [x 2])  1]) 0.5]

#        asinh: log(x + sqrt(x*x + 1))
#cLog  [(cAdd {x (cPow [(cAdd {(cPow [x 2])  1}) 0.5])})] -> cAsinh [x]
cLog2 [(cAdd {x (cPow [(cAdd {(cPow [x 2])  1}) 0.5])})] -> cMul (cAsinh [x]) CONSTANT_L2I

#        acosh: log(x + sqrt(x*x - 1))
#cLog  [(cAdd {x (cPow [(cAdd {(cPow [x 2]) -1}) 0.5])})] -> cAcosh [x]
cLog2 [(cAdd {x (cPow [(cAdd {(cPow [x 2]) -1}) 0.5])})] -> cMul (cAcosh [x]) CONSTANT_L2I

#        atanh(x):   log( (1+x) / (1-x)) / 2
#        2*atanh(x): log( (1+x) / (1-x))
#cLog  [(cMul {(cAdd {1 x}) (cPow [(cAdd {1 (cMul {-1 x})}) -1])})] -> cMul [(cAtanh [x]) 2]
cLog2 [(cMul {(cAdd {1 x}) (cPow [(cAdd {1 (cMul {-1 x})}) -1])})] -> cMul [(cAtanh [x]) *(2 CONSTANT_L2I)]

#        atanh(y*x):   log( (1+y*x) / (1+(-1*y*x))) / 2
#        2*atanh(y*x): log( (1+y*x) / (1+(-1*y*x)))

#        atanh(5*x):   log( (1+5*x) / (1+(-5*x))) / 2
#        2*atanh(5*x): log( (1+5*x) / (1+(-5*x)))

#        atanh(y+x):   log( (1+y+x) / (1+(-1*(y+x)))) / 2
#        2*atanh(y+x): log( (1+y+x) / (1+(-1*(y+x))))

cLog2 [(cMul {(cAdd {(cMul {% x}) 1}) (cPow [(cAdd {(cMul {-% x}) 1}) -1])})] -> cMul [(cAtanh [(cMul % x)]) *(2 CONSTANT_L2I)]

# atanh(x)              = log2( ((x*-2)+1) / ((x*2)-1) ) * log(2)/2
# atanh(x)*2/log(2)     = log2( ((x*-2)+1) / ((x*2)-1) )
# y^(atanh(x)*2/log(y)) = ((x*-y)+1) / ((x*y)-1)

#cMul (cAdd {(cMul {x %}) 1}) (cPow [(cAdd {(cMul {x -%}) -1}) -1]) : (cPow [% (cMul (cAtanh[x]) *(2 /LOG(%)))])
cMul (cPow [(cAdd {(cMul {x %}) &}) -1]) (cAdd {(cMul {x -%}) +(2 -&)}) : (cPow [-% (cMul (cAtanh[(cAdd (cMul -% x) & -1)]) *(2 /LOG(-%)))])



# cot(pi/2 - x) = 1/tan(pi/2 - x) = tan(x)
#                   tan(pi/2 - x) = 1/tan(x)
#                      reverse is probably better
#                      but cot() isn't exactly bad, so keep it
#cPow [(cTan[x]) -1] -> cTan [(cAdd [CONSTANT_PIHALF (cMul [-1 x])])]

cMul (cTan [(cAdd {CONSTANT_PIHALF (cMul -1 <1>)})]) (cTan [(cMul <1>)]) : 1


# tan(atan(x)) = x
cTan [(cAtan [x])] -> x

[FINAL2]

# x * CONSTANT_DR = cDeg(x)
cMul  CONSTANT_DR <1> -> cDeg [(cMul <1>)]
# Note: This may produce one-operand cMul lists.
# Those are treated properly in the bytecode generation,
# and do not need to be handled by [INTERMEDIATE].

# x * CONSTANT_RD = cRad(x)
cMul  CONSTANT_RD <1> -> cRad [(cMul <1>)]

# log(x) / CONSTANT_L10  = log10(x)
cMul        (cLog [x])      CONSTANT_L10I  :        (cLog10 [x])
cMul (cPow [(cLog [x]) -1]) CONSTANT_L10   : (cPow [(cLog10 [x]) -1])

# log(x) / CONSTANT_L2 = log2(x)
cMul        (cLog [x])      CONSTANT_L2I  :        (cLog2 [x])
cMul (cPow [(cLog [x]) -1]) CONSTANT_L2   : (cPow [(cLog2 [x]) -1])

# log2(x) * CONSTANT_L2 = log(x)
cMul        (cLog2 [x])      CONSTANT_L2   :        (cLog [x])
cMul (cPow [(cLog2 [x]) -1]) CONSTANT_L2I  : (cPow [(cLog [x]) -1])

# log2(x) * CONSTANT_L10B = log10(x)
cMul        (cLog2 [x])      CONSTANT_L10B   :        (cLog10 [x])
cMul (cPow [(cLog2 [x]) -1]) CONSTANT_L10BI  : (cPow [(cLog10 [x]) -1])

#cAtan [(cMul y ~x <1> ~<2>)] -> cAtan2 [(cMul y <1>) (cMul x <2>)]

# inverted pow is better expressed with negated exponent
#  this makes x^-acos(x) , which is turned into 1/x^acos(x)
#  turn back into x^-acos(x)
# FIXME: only if y is not integer constant
#cMul ~(cPow [x y]) : (cPow [x (cAdd [~y])])

cPow [ (cSin[x]) $ ] : (cCsc[x]) -$
cPow [ (cCos[x]) $ ] : (cSec[x]) -$
cPow [ (cTan[x]) $ ] : (cCot[x]) -$
#cPow [ (cCsc[x]) $ ] : (cSin[x]) -$
#cPow [ (cSec[x]) $ ] : (cCos[x]) -$
#cPow [ (cCot[x]) $ ] : (cTan[x]) -$
